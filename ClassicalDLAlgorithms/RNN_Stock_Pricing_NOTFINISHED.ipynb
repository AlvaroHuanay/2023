{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-18T15:29:00.159727Z","iopub.execute_input":"2023-06-18T15:29:00.160302Z","iopub.status.idle":"2023-06-18T15:29:00.198901Z","shell.execute_reply.started":"2023-06-18T15:29:00.160268Z","shell.execute_reply":"2023-06-18T15:29:00.197697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:29:00.203648Z","iopub.execute_input":"2023-06-18T15:29:00.203982Z","iopub.status.idle":"2023-06-18T15:29:01.809793Z","shell.execute_reply.started":"2023-06-18T15:29:00.203954Z","shell.execute_reply":"2023-06-18T15:29:01.808669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oil_df=pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/oil.csv\")\nsample_submission_df=pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/sample_submission.csv\")\nholiday_events_df=pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\")\nstores_df=pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/stores.csv\")\ntrain_df=pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/train.csv\")\ntest_df=pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/test.csv\")\ntransactions_df=pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/transactions.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:29:01.812057Z","iopub.execute_input":"2023-06-18T15:29:01.812493Z","iopub.status.idle":"2023-06-18T15:29:05.303297Z","shell.execute_reply.started":"2023-06-18T15:29:01.812451Z","shell.execute_reply":"2023-06-18T15:29:05.302088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nDaily oil price. Includes values during both the train and test data timeframes.\n(Ecuador is an oil-dependent country and it's economical health is highly vulnerable\nto shocks in oil prices.)\n\nTake into account that the stock market does not work on weekends. Therefore we will\nhave jumps in days that do not correspond to datapoints directly, i.e., the price\nlist with 1000 prices does not correspond to 1000 prices in row days, it is\nthe price of the first 1000 workdays (discarding holidays, festivities and weekends)\n\"\"\"\nprint(\"This pandas is oil_df\\n\")\nprint(\"Size of the pandas is: \", oil_df.shape)\nprint(\" \")\nprint(\"The number of null elements (NaN) is: \\n\", oil_df.isnull().sum())\nprint(\" \")\n#It can be observed the only missing data is in oil_df\nprint(\"Information is: \\n\", oil_df.describe())\nprint(\" \")\n\n\n#Let´s add two columns for our interest:\noil_df[\"Month\"]=oil_df[\"date\"].apply(lambda x: int(x.split(\"-\")[1]))\noil_df[\"Year\"]=oil_df[\"date\"].apply(lambda x: int(x.split(\"-\")[0].strip()))\noil_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:29:23.282896Z","iopub.execute_input":"2023-06-18T15:29:23.283285Z","iopub.status.idle":"2023-06-18T15:29:23.335716Z","shell.execute_reply.started":"2023-06-18T15:29:23.283252Z","shell.execute_reply":"2023-06-18T15:29:23.334631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nObserve empty areas, that is the NaN part, we will make a tendency to make those missing data be filled\nWe will fill this data too with an RNN, training with the first not NaN and then filling the NaN part.\nWe will divide it in sections\n\"\"\"\n\n# xtick_positions=[0, 1*365, 2*365, 3*365, 4*365] #We can´t do this because of what we explained about weekdays and holidays before\nxtick_positions=[oil_df.loc[oil_df['Year'] == 2013].index[0], oil_df.loc[oil_df['Year'] == 2014].index[0],\n                 oil_df.loc[oil_df['Year'] == 2015].index[0], oil_df.loc[oil_df['Year'] == 2016].index[0],\n                 oil_df.loc[oil_df['Year'] == 2017].index[0]]\nxtick_labels=[\"2013\", \"2014\", \"2015\", \"2016\", \"2017\"]\n\nplt.plot(oil_df[\"date\"], oil_df[\"dcoilwtico\"], label=\"Oil price\")\nplt.title(\"Oil price per day in Ecuador (2013-2017)\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Oil price\")\nplt.xticks(xtick_positions, xtick_labels)\nplt.legend()\nplt.grid(\"True\")","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:30:00.867910Z","iopub.execute_input":"2023-06-18T15:30:00.868270Z","iopub.status.idle":"2023-06-18T15:30:01.296747Z","shell.execute_reply.started":"2023-06-18T15:30:00.868242Z","shell.execute_reply":"2023-06-18T15:30:01.295719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nChecking both plots, we can corroborate that the years that had the highest oil price were 2013 and 2014.\nAfter year 2014 the price plumbed. Something happened.\n\"\"\"\n\n\ndcoilwtico_hist=pd.concat([oil_df[oil_df.Year==2013]['dcoilwtico'],oil_df[oil_df.Year==2014]['dcoilwtico'], oil_df[oil_df.Year==2015]['dcoilwtico'], oil_df[oil_df.Year==2016]['dcoilwtico'], oil_df[oil_df.Year==2017]['dcoilwtico']],axis=1)\ndcoilwtico_hist.columns=[\"2013\", \"2014\", \"2015\", \"2016\", \"2017\"]\ndcoilwtico_hist.plot(kind=\"hist\", bins=30, edgecolor=\"black\", figsize=(7,5), alpha=0.35)\nplt.xlabel(\"dcoilwtico\")\nplt.grid(\"True\")\nplt.legend(ncol=3)\nplt.title(\"dcoilwtico Frequency per year\")","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:30:12.529599Z","iopub.execute_input":"2023-06-18T15:30:12.529996Z","iopub.status.idle":"2023-06-18T15:30:13.302137Z","shell.execute_reply.started":"2023-06-18T15:30:12.529966Z","shell.execute_reply":"2023-06-18T15:30:13.301134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"This pandas is holiday_evets_df\\n\")\nprint(\"Size of the pandas is: \", holiday_events_df.shape)\nprint(\" \")\nprint(\"The number of null elements (NaN) is: \\n\", holiday_events_df.isnull().sum())\nprint(\" \")\nprint(\"Information is: \\n\", holiday_events_df.describe())\nprint(\" \")\n\n\n#Let´s add two columns for our interest:\nholiday_events_df[\"Month\"]=holiday_events_df[\"date\"].apply(lambda x: int(x.split(\"-\")[1]))\nholiday_events_df[\"Year\"]=holiday_events_df[\"date\"].apply(lambda x: int(x.split(\"-\")[0].strip()))\nholiday_events_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:30:35.085386Z","iopub.execute_input":"2023-06-18T15:30:35.085781Z","iopub.status.idle":"2023-06-18T15:30:35.124552Z","shell.execute_reply.started":"2023-06-18T15:30:35.085749Z","shell.execute_reply":"2023-06-18T15:30:35.123359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# holiday_events_df[\"description\"].unique() # Here we found that there was a worldwide futbol competition and an earthquake\n# Let´s check the dates\n\nlongest_events_dates_df=pd.concat([holiday_events_df[holiday_events_df.description.str.contains(\"Mundial\")], holiday_events_df[holiday_events_df.description.str.contains(\"Terremoto\")]])\n#These dates can be of interest. Let´s observe when these two events started:\nprint(\"Mundial in Brazil started: {} and Earthquake stated: {}\".format(holiday_events_df.iloc[holiday_events_df[holiday_events_df.description.str.contains(\"Mundial\")].index[0]][\"date\"],\n                                                                      holiday_events_df.iloc[holiday_events_df[holiday_events_df.description.str.contains(\"Terremoto\")].index[0]][\"date\"]))","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:30:46.149397Z","iopub.execute_input":"2023-06-18T15:30:46.150293Z","iopub.status.idle":"2023-06-18T15:30:46.162823Z","shell.execute_reply.started":"2023-06-18T15:30:46.150251Z","shell.execute_reply":"2023-06-18T15:30:46.161678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In 2014 they were almost at their top in oil price and in 2016 they were almost at the bottom. The earthquake undermined the oil price at that moment with the last dump.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nstores.csv:\n\nStore metadata, including city, state, type, and cluster.\ncluster is a grouping of similar stores.\n\"\"\"\nprint(\"This pandas is stores_df\\n\")\nprint(\"Size of the pandas is: \", stores_df.shape)\nprint(\" \")\nprint(\"The number of null elements (NaN) is: \\n\", stores_df.isnull().sum())\nprint(\" \")\nprint(\"Information is: \\n\", stores_df.describe())\nprint(\" \")\n\nstores_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:31:03.910030Z","iopub.execute_input":"2023-06-18T15:31:03.910415Z","iopub.status.idle":"2023-06-18T15:31:03.934755Z","shell.execute_reply.started":"2023-06-18T15:31:03.910385Z","shell.execute_reply":"2023-06-18T15:31:03.933449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ntrain.csv:\n\nThe training data, comprising time series of features store_nbr, family, and onpromotion as well as the target sales.\n\nstore_nbr: identifies the store at which the products are sold.\nfamily: identifies the type of product sold.\nsales: gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips).\nonpromotion: gives the total number of items in a product family that were being promoted at a store at a given date.\n\"\"\"\n\nprint(\"This pandas is train_df\\n\")\nprint(\"Size of the pandas is: \", train_df.shape)\nprint(\" \")\nprint(\"The number of null elements (NaN) is: \\n\", train_df.isnull().sum())\nprint(\" \")\nprint(\"Information is: \\n\", train_df.describe())\nprint(\" \")\n\n#Let´s add two columns for our interest:\ntrain_df[\"Month\"]=train_df[\"date\"].apply(lambda x: int(x.split(\"-\")[1]))\ntrain_df[\"Year\"]=train_df[\"date\"].apply(lambda x: int(x.split(\"-\")[0]))\ntrain_df[\"Day\"]=train_df[\"date\"].apply(lambda x: int(x.split(\"-\")[2]))\n                                                         \ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:31:17.456848Z","iopub.execute_input":"2023-06-18T15:31:17.457214Z","iopub.status.idle":"2023-06-18T15:31:25.117851Z","shell.execute_reply.started":"2023-06-18T15:31:17.457187Z","shell.execute_reply":"2023-06-18T15:31:25.117072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtick_positions=[train_df.loc[train_df['Year'] == 2013].index[0], train_df.loc[train_df['Year'] == 2014].index[0],\n                 train_df.loc[train_df['Year'] == 2015].index[0], train_df.loc[train_df['Year'] == 2016].index[0],\n                 train_df.loc[train_df['Year'] == 2017].index[0]]\nxtick_labels=[\"2013\", \"2014\", \"2015\", \"2016\", \"2017\"]\n\nplt.figure(figsize=(18,5))\nplt.plot(np.arange(len(train_df[(train_df.store_nbr==1) & (train_df.family==\"AUTOMOTIVE\")].sales)),\n         train_df[(train_df.store_nbr==1) & (train_df.family==\"AUTOMOTIVE\")].sales, label=\"AUTOMOTIVE nbr = 1\")\nplt.title(\"Sales price per day in AUTOMOTIVE nbr=1 (2013-2017)\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"AUTOMOTIVE sales price\")\n#plt.xticks(xtick_positions, xtick_labels)\nplt.legend()\nplt.grid(\"True\")","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:32:11.208142Z","iopub.execute_input":"2023-06-18T15:32:11.208554Z","iopub.status.idle":"2023-06-18T15:32:12.434310Z","shell.execute_reply.started":"2023-06-18T15:32:11.208519Z","shell.execute_reply":"2023-06-18T15:32:12.433207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nbr_sales=train_df.groupby([\"store_nbr\"], as_index=False).agg({\"sales\":\"sum\"})\n\nplt.figure(figsize=(18,5))\nsns.barplot(x=nbr_sales.index, y=\"sales\", data=nbr_sales)\nplt.title(\"Sales per Store\")\nplt.xlabel(\"Store\")\nplt.ylabel(\"Sales\")\n#plt.xticks(xtick_positions, xtick_labels)\nplt.legend()\nplt.grid(\"True\")","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:32:52.420169Z","iopub.execute_input":"2023-06-18T15:32:52.420521Z","iopub.status.idle":"2023-06-18T15:32:53.385836Z","shell.execute_reply.started":"2023-06-18T15:32:52.420495Z","shell.execute_reply":"2023-06-18T15:32:53.384596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_year_table=train_df.groupby([\"Year\"], as_index=False).agg({\"sales\":\"sum\"})\n#as_index=False to set the Year as a column and have another index instead of Year\nplt.bar(new_year_table.Year, new_year_table.sales)\nplt.title(\"Sales per year\")\nplt.ylabel(\"Sales\")\nplt.xlabel(\"Year\")\nplt.grid(\"True\")","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:33:08.607320Z","iopub.execute_input":"2023-06-18T15:33:08.607805Z","iopub.status.idle":"2023-06-18T15:33:08.984413Z","shell.execute_reply.started":"2023-06-18T15:33:08.607766Z","shell.execute_reply":"2023-06-18T15:33:08.981599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nrows=2\nncols=3\nfig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 8))\nplt.subplots_adjust(wspace=0.35, hspace=0.35)\ni=0 #i=row\nj=0 #j=column\nfor year in train_df[\"Year\"].unique():\n    \n    selected_subtable=train_df[train_df[\"Year\"]==year]\n    selected_subtable=selected_subtable.groupby([\"Month\"], as_index=False).agg({\"sales\":\"sum\"})\n    \n    axs[i,j].bar(selected_subtable.Month, selected_subtable.sales)\n    axs[i,j].set_xticks(selected_subtable.Month)\n    axs[i,j].set_xticklabels(selected_subtable.Month, rotation=50)\n    axs[i,j].set_title(\"Sales per month {}\".format(str(year)))\n    axs[i,j].set_xlabel(\"Month\")\n    axs[i,j].set_ylabel(\"Sales\")\n    axs[i,j].grid(True)\n    j=j+1\n    if j==ncols:\n        i=i+1\n        j=0","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:33:18.027508Z","iopub.execute_input":"2023-06-18T15:33:18.028666Z","iopub.status.idle":"2023-06-18T15:33:19.981130Z","shell.execute_reply.started":"2023-06-18T15:33:18.028630Z","shell.execute_reply":"2023-06-18T15:33:19.980000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let´s analyze the best (apparently) year 2015","metadata":{}},{"cell_type":"code","source":"store_train_df=train_df.merge(stores_df, how=\"inner\", on=\"store_nbr\")\n\"\"\"\nIt follows a very similar style as with SQL. It performs an inner join between two DataFrames\ntrain_df and stores_df based on the common column \"store_nbr\".\n\nThe resulting DataFrame \"store_train_df\" will contain the combined data from both DataFrames,\nwhere the rows with matching \"store_nbr\" values are merged together.\n\"\"\"\n\n#Let´s check the year 2015\nstore_train_2015_df=store_train_df[store_train_df.Year==2015]\nstore_train_2015_df.columns","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:41:43.644270Z","iopub.execute_input":"2023-06-18T15:41:43.644687Z","iopub.status.idle":"2023-06-18T15:41:45.291220Z","shell.execute_reply.started":"2023-06-18T15:41:43.644652Z","shell.execute_reply":"2023-06-18T15:41:45.289962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_name=[\"family\", \"onpromotion\", \"state\", \"city\", \"type\", \"cluster\", \"store_nbr\"]\nplt.subplots_adjust(wspace=0.35, hspace=0.35)\nfor name in list_name:\n    plt.figure(figsize=(15,5))\n    sales_train_2015_df=store_train_2015_df.groupby([name], as_index=False).agg({\"sales\":\"sum\"})\n    sns.barplot(x=sales_train_2015_df[name], y=\"sales\", data=sales_train_2015_df)\n    plt.title(\"Sales per {} year {}\".format(name,2015))\n    plt.ylabel(\"Sales\")\n    plt.xlabel(name)\n    if name==\"state\" or name==\"city\" or name==\"family\":\n        plt.xticks(rotation=70)\n    elif name==\"onpromotion\":\n        plt.xticks(range(0, len(sales_train_2015_df[name]), 10))\n    else:\n        plt.xticks(rotation=0)\n    plt.grid(\"True\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:42:40.204025Z","iopub.execute_input":"2023-06-18T15:42:40.204430Z","iopub.status.idle":"2023-06-18T15:42:44.740458Z","shell.execute_reply.started":"2023-06-18T15:42:40.204397Z","shell.execute_reply":"2023-06-18T15:42:44.739156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nLet´s analyze the transactions_df\nLet´s convert the date column to \"datetime\" to apply \".dt\" and obtain day of the week.\n\"\"\"\ntransactions_df[\"date\"]=pd.to_datetime(transactions_df[\"date\"])\ntransactions_df[\"Day_of_week\"]=transactions_df[\"date\"].dt.day_name()\ntransactions_df[\"Month\"]=transactions_df[\"date\"].dt.month\ntransactions_df[\"Year\"]=transactions_df[\"date\"].dt.year.astype(str)\ntransactions_df","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:47:14.057299Z","iopub.execute_input":"2023-06-18T15:47:14.057713Z","iopub.status.idle":"2023-06-18T15:47:14.165621Z","shell.execute_reply.started":"2023-06-18T15:47:14.057678Z","shell.execute_reply":"2023-06-18T15:47:14.164488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nLet´s do the same as before but with two input features to create a heatmap.\n\"\"\"\n\nlist_names=[\"Month\", \"Day_of_week\"]\nfor name in list_names:\n    plt.figure(figsize=(8,8))\n    new_subtable=transactions_df.groupby([name, \"Year\"], as_index=False).agg({\"transactions\":\"sum\"})\n    heat_map=new_subtable.pivot(name, \"Year\", \"transactions\")\n    sns.heatmap(heat_map, annot=True, linecolor=\"white\", linewidths=0.5, cmap=\"YlGnBu\")\n    plt.xlabel(\"Year\")\n    plt.ylabel(name)\n    plt.title(\"Number of transactions per {} and year\".format(name))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:47:31.148509Z","iopub.execute_input":"2023-06-18T15:47:31.148911Z","iopub.status.idle":"2023-06-18T15:47:32.088673Z","shell.execute_reply.started":"2023-06-18T15:47:31.148879Z","shell.execute_reply":"2023-06-18T15:47:32.087537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"holiday_train_df=train_df.merge(holiday_events_df, how=\"inner\", on=\"date\")\n\"\"\"\nPerforms an inner join between two DataFrames \"train_df\" and \"holiday_events_df\" based on the common column \"date\".\nhow=\"inner\" specifies that an inner join should be performed, meaning only the matching rows between the two DataFrames\nwill be included in the resulting DataFrame.\n\"\"\"\nholiday_train_df","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:51:02.910464Z","iopub.execute_input":"2023-06-18T15:51:02.911422Z","iopub.status.idle":"2023-06-18T15:51:03.238606Z","shell.execute_reply.started":"2023-06-18T15:51:02.911378Z","shell.execute_reply":"2023-06-18T15:51:03.237592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\n\nlist_name=[\"type\", \"locale\", \"locale_name\", \"transferred\"]\nlist_year=list(holiday_train_df.Year_y.unique())\ncolors=[\"blue\", \"red\", \"green\", \"yellow\", \"purple\"]\n\nfor year in list_year:\n    print(\"Year {}:\".format(year))\n    holiday_train_year_df = holiday_train_df[holiday_train_df.Year_y == year]\n\n    nrows=2\n    ncols=2\n    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 8))\n\n    i=0  # rows\n    j=0  # columns\n\n    for name in list_name:\n        hist_holiday_train_year_df = holiday_train_year_df.groupby([name], as_index=False).agg({\"sales\": \"sum\"})\n        axs[i,j].bar(hist_holiday_train_year_df[name], hist_holiday_train_year_df[\"sales\"], color=\"blue\", alpha=0.5, edgecolor=\"black\", linewidth=1.5)\n        axs[i,j].set_title(\"Sales per {} year {}\".format(name, year))\n        axs[i,j].set_ylabel(\"Sales\")\n        axs[i,j].set_xlabel(name)\n        if name==\"locale_name\":\n            axs[i,j].tick_params(axis=\"x\", rotation=70)\n        else:\n            axs[i,j].tick_params(axis=\"x\", rotation=0)\n        axs[i, j].grid(True)\n        j += 1\n        if j == ncols:\n            i += 1\n            j = 0\n\n    plt.tight_layout()\n    plt.show()\n    print(\"\\n\")\n    print(\"\\n\")\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:52:30.812646Z","iopub.execute_input":"2023-06-18T15:52:30.813020Z","iopub.status.idle":"2023-06-18T15:52:36.549635Z","shell.execute_reply.started":"2023-06-18T15:52:30.812991Z","shell.execute_reply":"2023-06-18T15:52:36.548528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature Engineering**","metadata":{}},{"cell_type":"markdown","source":"Feature engineering is the process of creating new features or modifying existing features in a dataset to improve the performance and effectiveness of machine learning models. It involves transforming raw data into a format that is more suitable for the model to learn from.","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:53:34.737144Z","iopub.execute_input":"2023-06-18T15:53:34.737529Z","iopub.status.idle":"2023-06-18T15:53:34.742780Z","shell.execute_reply.started":"2023-06-18T15:53:34.737497Z","shell.execute_reply":"2023-06-18T15:53:34.741581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.hist(train_df.sales, bins=60)\nplt.grid(True)\nplt.show()\nprint(\"The data is too skewed, let´s apply log transformation to reduce the skewness.\")\n\n\n\nlogsale=[]\nfor sale in tqdm(train_df.sales):\n    sale=np.log1p(float(sale))\n    logsale.append(sale)\n\ntrain_df[\"logsale\"]=logsale\n\nplt.hist(train_df.logsale, bins=30)\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-18T15:54:44.432851Z","iopub.execute_input":"2023-06-18T15:54:44.433347Z","iopub.status.idle":"2023-06-18T15:54:50.534019Z","shell.execute_reply.started":"2023-06-18T15:54:44.433307Z","shell.execute_reply":"2023-06-18T15:54:50.532877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2023-06-18T16:00:01.399523Z","iopub.execute_input":"2023-06-18T16:00:01.399957Z","iopub.status.idle":"2023-06-18T16:00:01.404616Z","shell.execute_reply.started":"2023-06-18T16:00:01.399925Z","shell.execute_reply":"2023-06-18T16:00:01.403347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder=LabelEncoder()\ntrain_df[\"family\"]=encoder.fit_transform(train_df[\"family\"].values)\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2023-06-18T16:00:01.603926Z","iopub.execute_input":"2023-06-18T16:00:01.604305Z","iopub.status.idle":"2023-06-18T16:00:02.338258Z","shell.execute_reply.started":"2023-06-18T16:00:01.604274Z","shell.execute_reply":"2023-06-18T16:00:02.337141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_name=[\"family\", \"city\", \"state\", \"type\"]\nfor name in label_name:\n    store_train_df[name]=encoder.fit_transform(store_train_df[name].values)\nstore_train_df","metadata":{"execution":{"iopub.status.busy":"2023-06-18T16:00:04.776337Z","iopub.execute_input":"2023-06-18T16:00:04.776754Z","iopub.status.idle":"2023-06-18T16:00:07.828372Z","shell.execute_reply.started":"2023-06-18T16:00:04.776718Z","shell.execute_reply":"2023-06-18T16:00:07.827242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2023-06-18T16:16:11.512269Z","iopub.execute_input":"2023-06-18T16:16:11.512807Z","iopub.status.idle":"2023-06-18T16:16:11.530961Z","shell.execute_reply.started":"2023-06-18T16:16:11.512763Z","shell.execute_reply":"2023-06-18T16:16:11.529792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2023-06-18T16:16:52.238402Z","iopub.execute_input":"2023-06-18T16:16:52.238812Z","iopub.status.idle":"2023-06-18T16:16:52.258709Z","shell.execute_reply.started":"2023-06-18T16:16:52.238776Z","shell.execute_reply":"2023-06-18T16:16:52.257603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Size of the pandas is: \", test_df.shape)\nprint(\" \")\nprint(\"The number of null elements (NaN) is: \\n\", test_df.isnull().sum())\nprint(\" \")\nprint(\"Information is: \\n\", test_df.describe())\nprint(\" \")\ntest_df.head()\n#We have to predict sales. We will use a RNN (LSTM)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:24:28.819193Z","iopub.execute_input":"2023-06-08T08:24:28.819672Z","iopub.status.idle":"2023-06-08T08:24:28.876213Z","shell.execute_reply.started":"2023-06-08T08:24:28.819638Z","shell.execute_reply":"2023-06-08T08:24:28.874674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Size of the pandas is: \", transactions_df.shape)\nprint(\" \")\nprint(\"The number of null elements (NaN) is: \\n\", transactions_df.isnull().sum())\nprint(\" \")\nprint(\"Information is: \\n\", transactions_df.describe())\nprint(\" \")\ntransactions_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:25:00.048828Z","iopub.execute_input":"2023-06-08T08:25:00.049256Z","iopub.status.idle":"2023-06-08T08:25:00.109031Z","shell.execute_reply.started":"2023-06-08T08:25:00.049223Z","shell.execute_reply":"2023-06-08T08:25:00.107733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building the RNN**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, SimpleRNN","metadata":{"execution":{"iopub.status.busy":"2023-06-18T16:32:52.530011Z","iopub.execute_input":"2023-06-18T16:32:52.530404Z","iopub.status.idle":"2023-06-18T16:32:52.535185Z","shell.execute_reply.started":"2023-06-18T16:32:52.530374Z","shell.execute_reply":"2023-06-18T16:32:52.533997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Build the RNN architecture\nmodel=Sequential()\nmodel.add(SimpleRNN(units=32, input_shape(10,1)))\nmodel.add(Dense(units=1))\n\n#Compile model\nmodel.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n\n#Train the model\nmodel.fit(X,y, epochs=10, batch_size=32)","metadata":{},"execution_count":null,"outputs":[]}]}