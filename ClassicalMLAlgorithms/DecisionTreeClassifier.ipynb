{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_RI5l4E77F40"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import tools/libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "import urllib\n",
        "import io\n",
        "import pandas as pd\n",
        "import requests\n",
        "from decorator import get_init\n",
        "from numpy.lib.function_base import gradient"
      ],
      "metadata": {
        "id": "KhPAdA397Hl-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the data\n",
        "# Download the iris dataset from the internet\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
        "s = requests.get(url).content\n",
        "\n",
        "col_names=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"type\"]\n",
        "data = pd.read_csv(io.StringIO(s.decode('utf-8')), skiprows=1, header=None, names=col_names)\n",
        "#data=pd.read_csv(pd.compat.StringIO(raw_data), skiprows=1, header=None, names=col_names)\n",
        "data.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "uPDV72W17HiU",
        "outputId": "745f4db7-19fa-462d-c727-5e84eab35cb3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sepal_length  sepal_width  petal_length  petal_width         type\n",
              "0           4.9          3.0           1.4          0.2  Iris-setosa\n",
              "1           4.7          3.2           1.3          0.2  Iris-setosa\n",
              "2           4.6          3.1           1.5          0.2  Iris-setosa\n",
              "3           5.0          3.6           1.4          0.2  Iris-setosa\n",
              "4           5.4          3.9           1.7          0.4  Iris-setosa\n",
              "5           4.6          3.4           1.4          0.3  Iris-setosa\n",
              "6           5.0          3.4           1.5          0.2  Iris-setosa\n",
              "7           4.4          2.9           1.4          0.2  Iris-setosa\n",
              "8           4.9          3.1           1.5          0.1  Iris-setosa\n",
              "9           5.4          3.7           1.5          0.2  Iris-setosa"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0e2ab239-6363-4c37-ad1b-c15b04bbec5b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.3</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4.4</td>\n",
              "      <td>2.9</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e2ab239-6363-4c37-ad1b-c15b04bbec5b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0e2ab239-6363-4c37-ad1b-c15b04bbec5b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0e2ab239-6363-4c37-ad1b-c15b04bbec5b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Node Class\n",
        "class Node():\n",
        "  def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
        "    \"\"\"\n",
        "    Constructor:\n",
        "    We use object oriented programming because it is easier to assign atributes to each of the nodes\n",
        "    Node is a class object tha always has these attributes.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    #for decision node\n",
        "    self.feature_index=feature_index #Integer representing the index of the feature used for splitting\n",
        "    self.threshold=threshold #cutogg value for the feature used to split the data at that particular node in the tree\n",
        "    self.left=left #Left child of this node\n",
        "    self.right=right #Right child of this node\n",
        "    self.info_gain=info_gain #Information gained by splitting this node\n",
        "\n",
        "    #for leaf node\n",
        "    self.value=value #Predicted value at this node"
      ],
      "metadata": {
        "id": "Nt7js4I27Hd0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTreeClassifier():\n",
        "    def __init__(self, min_samples_split, max_depth):\n",
        "        ''' \n",
        "        root: root node represents the entire dataset,\n",
        "        and all subsequent nodes represent subsets of the data based on feature splits.\n",
        "\n",
        "        min_samples_split: determines the minimum number of samples required to make a split at an internal node.\n",
        "        If it has fewer than min_saples_split samples, the algorithm will not split that node any further and will become a leaf node.\n",
        "        The appropriate value depends on the size and complexity of the dataset and should be chosen through cross-validation.\n",
        "\n",
        "        max_depth: determines the depth of the decision tree. It is the length of the longest path from the root to the leaf node.\n",
        "        It determines how many levels deep a decision tree can go\n",
        "\n",
        "        Remember that a node has always 2 child nodes per level.\n",
        "        '''\n",
        "        \n",
        "        # initialize the root of the tree \n",
        "        self.root = None #When the tree is initialized, there is no data to use for the root node\n",
        "        \n",
        "        # stopping conditions\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        \n",
        "    def build_tree(self, dataset, curr_depth=0):\n",
        "        '''\n",
        "        Recursive function that implements the construction of the\n",
        "        decision tree. It takes the input training data X and y, the\n",
        "        current depth of the tree and maximum depth.\n",
        "\n",
        "        At each call the function selects the best feature and corresponding\n",
        "        split point that will produce the highest information gain using the\n",
        "        find_best_split function. It splits the data based on the selected feature\n",
        "        and split point creating a new node of the tree.\n",
        "\n",
        "        If the maximum depth has not been reached and the current node has enough\n",
        "        samples to split, the function recursively calls itself to build the left\n",
        "        and right sub-trees. If either of these conditions is not met, the current\n",
        "        node becomes a leaf node and its predicted class is set to the majority class\n",
        "        of the samples in the node.\n",
        "\n",
        "        It then returns the root node of the decision tree, which can be used to make\n",
        "        predictions on new data.\n",
        "        ''' \n",
        "        \n",
        "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
        "        #[:,:-1] select all rows and columns except the last column (features)\n",
        "        #[:,-1] select all rows and only the last column (labels/target variables we want to predict)\n",
        "        num_samples, num_features = np.shape(X)\n",
        "        \n",
        "        # split until stopping conditions are met\n",
        "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
        "            # find the best split\n",
        "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
        "            # check if information gain is positive\n",
        "            if best_split[\"info_gain\"]>0:\n",
        "                # recur left\n",
        "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
        "                # recur right\n",
        "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
        "                # return new decision node\n",
        "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
        "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
        "                # Returns node with the index of the feature that produced the best split in the current node\n",
        "                # Its threshold, the left/right subtree, the information gain resulting from the split\n",
        "        \n",
        "        # compute leaf node\n",
        "        leaf_value = self.calculate_leaf_value(Y)\n",
        "        # return leaf node\n",
        "        return Node(value=leaf_value)\n",
        "    \n",
        "    def get_best_split(self, dataset, num_samples, num_features):\n",
        "        '''\n",
        "        Finds the best split for a given dataset. Takes as input the dataset, number of samples, and features.\n",
        "        Creates a dictionary to store the details of the best split.\n",
        "\n",
        "        The function loops over all features in the dataset and gets all the unique values of the current feature.\n",
        "        For each unique value, it splits the dataset into left/right child nodes based on whether the feature\n",
        "        is less than or greater than the threshold.\n",
        "\n",
        "        For each split, the function computes the information gain using the Gini index and updates the best_split\n",
        "        dictionary if the current information gain is greater thn the previous maximum information gain.\n",
        "\n",
        "        Finally, the function returns the best_split dictionary, which contains the parameters for the best_split.\n",
        "        '''\n",
        "        \n",
        "        # dictionary to store the best split\n",
        "        best_split = {}\n",
        "        max_info_gain = -float(\"inf\")\n",
        "        \n",
        "        # loop over all the features\n",
        "        for feature_index in range(num_features):\n",
        "            feature_values = dataset[:, feature_index] # Grab the values of that specific feature_index\n",
        "            possible_thresholds = np.unique(feature_values) # Set as possible thresholds to study the unique values of this new dataset \"feature_values\"\n",
        "            # loop over all the feature values present in the data\n",
        "            for threshold in possible_thresholds:\n",
        "                # get current split\n",
        "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
        "                # check if childs are not null\n",
        "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
        "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
        "                    # compute information gain\n",
        "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
        "                    # if the current info_gain is more than the previous one, update parameters of dictionary:\n",
        "                    if curr_info_gain>max_info_gain:\n",
        "                        \n",
        "                        best_split[\"feature_index\"] = feature_index\n",
        "                        best_split[\"threshold\"] = threshold\n",
        "                        best_split[\"dataset_left\"] = dataset_left\n",
        "                        best_split[\"dataset_right\"] = dataset_right\n",
        "                        best_split[\"info_gain\"] = curr_info_gain\n",
        "                        max_info_gain = curr_info_gain\n",
        "                        \n",
        "        # return best split\n",
        "        return best_split\n",
        "    \n",
        "    def split(self, dataset, feature_index, threshold):\n",
        "        '''\n",
        "        Method that grabs the original dataset, feature index and threshold.\n",
        "        It returns a tuple (dataset_left, dataset_right) which are two new numpy arrays\n",
        "        containing:\n",
        "\n",
        "        dataset_left (right): An array that contains all the rows from dataset where the value\n",
        "        of the feature at feature_index is <= (>) to threshold.\n",
        "        '''\n",
        "        \n",
        "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
        "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
        "        return dataset_left, dataset_right\n",
        "    \n",
        "    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):\n",
        "        '''\n",
        "        Computes information gain for a given split.\n",
        "\n",
        "        parent: Target variable of the parent node.\n",
        "        l/r_child: Target variables of the left/right child nodes from split.\n",
        "        mode: Mode to compute the entropy\n",
        "        '''\n",
        "        \n",
        "        weight_l = len(l_child) / len(parent)\n",
        "        weight_r = len(r_child) / len(parent)\n",
        "        # Counts the proportion of points that belong to the left (right) child with respect the previous parent node\n",
        "\n",
        "\n",
        "        if mode==\"gini\":\n",
        "            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n",
        "            # Uses gini mode to compute information gain\n",
        "        else:\n",
        "            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
        "            # Uses default mode to compute information gain\n",
        "        return gain\n",
        "    \n",
        "    def entropy(self, y):\n",
        "        '''\n",
        "        Method that computes entropy of a set of labels y.\n",
        "        Entropy is a measure of impurity of a set. The entropy is at its minimum (0) when all labels in the set belong\n",
        "        to the same class (the set is pure), and it is at its maximum (1) when the set is equally divided among all possible labels.\n",
        "        \n",
        "        Finally, it returns the entropy.\n",
        "        '''\n",
        "        \n",
        "        class_labels = np.unique(y)\n",
        "        entropy = 0\n",
        "        for cls in class_labels:\n",
        "            p_cls = len(y[y == cls]) / len(y)\n",
        "            # len(y[y==cls]) prompts the number of rows that are of type \"cls\"\n",
        "            entropy += -p_cls * np.log2(p_cls)\n",
        "        return entropy\n",
        "    \n",
        "    def gini_index(self, y):\n",
        "        '''\n",
        "        Calculates the gini index of a set of class labels. It is a measure of impurity of a set of labels.\n",
        "        0 means completely pure (all labels in the set are the same) and 1 completely impure (labels in the set are evenly distributed)\n",
        "        \n",
        "        Returns the gini_index.\n",
        "        '''\n",
        "        \n",
        "        class_labels = np.unique(y)\n",
        "        gini = 0\n",
        "        for cls in class_labels:\n",
        "            p_cls = len(y[y == cls]) / len(y) #prob. of that class\n",
        "            gini += p_cls**2\n",
        "        return 1 - gini\n",
        "        \n",
        "    def calculate_leaf_value(self, Y):\n",
        "        '''\n",
        "        Returns the label that appears the most frequently in the given set of labels (Y) of that node.\n",
        "        This most frequent label will be assigned to the leaf node.\n",
        "        '''\n",
        "        \n",
        "        Y = list(Y)\n",
        "        return max(Y, key=Y.count)\n",
        "    \n",
        "    def print_tree(self, tree=None, indent=\" \"):\n",
        "        '''\n",
        "        Prints the decision tree structure in a readable format.\n",
        "        '''\n",
        "        \n",
        "        if not tree:\n",
        "            tree = self.root\n",
        "\n",
        "        if tree.value is not None:\n",
        "            print(tree.value)\n",
        "\n",
        "        else:\n",
        "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
        "            print(\"%sleft:\" % (indent), end=\"\")\n",
        "            self.print_tree(tree.left, indent + indent)\n",
        "            print(\"%sright:\" % (indent), end=\"\")\n",
        "            self.print_tree(tree.right, indent + indent)\n",
        "    \n",
        "    def fit(self, X, Y):\n",
        "        '''\n",
        "        Trains the decision tree on a given concatenated dataset X+Y. Builds a tree using this dataset\n",
        "        finding the best split at each node and uses the root attribute to set it to the root node of\n",
        "        the constructed decision tree. Then, this decision tree is used for prediction on new data.\n",
        "\n",
        "        Assigning the root attribute of the DecisionTreeClassifier() object to the root node of the\n",
        "        decision tree means that the object now has access to the entire decision tree structure and\n",
        "        can use it to make predictions on new data.\n",
        "\n",
        "        The root node contains all the information needed to classify new samples by traversing the\n",
        "        tree and making decisions at each internal node based on the feature values of the new sample.\n",
        "\n",
        "        The best parameters that were learned during training, such as optimal feature to split on at\n",
        "        each internal node and the threshold for that feature, are encoded in the decision tree structure.\n",
        "        The root node represents the first decision point in the tree, and subsequent decisions are made\n",
        "        by following the appropriate branches until a leaf node is reached, which corresponds to the final\n",
        "        classification for the input sample.\n",
        "        '''\n",
        "        \n",
        "        dataset = np.concatenate((X, Y), axis=1)\n",
        "        self.root = self.build_tree(dataset)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Takes dataset X and predicts the class labels for each sample in the dataset using the decision tree\n",
        "        that has been trained earlier.\n",
        "\n",
        "        It returns a list of predicted labels.\n",
        "        '''\n",
        "        \n",
        "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
        "        return preditions\n",
        "    \n",
        "    def make_prediction(self, x, tree):\n",
        "        '''\n",
        "        Predicts the label of a single data point by traversing down the decision tree until a leaf node is reached.\n",
        "        The prediction is made based on the majority class of the training examples that reach that leaf node.\n",
        "        '''\n",
        "        \n",
        "        if tree.value!=None: return tree.value\n",
        "        feature_val = x[tree.feature_index]\n",
        "        if feature_val<=tree.threshold:\n",
        "            return self.make_prediction(x, tree.left)\n",
        "        else:\n",
        "            return self.make_prediction(x, tree.right)"
      ],
      "metadata": {
        "id": "76m4L7Bt7vgw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train/Test split\n",
        "\n",
        "X=data.iloc[:,:-1].values\n",
        "Y=data.iloc[:,-1].values.reshape(-1,1)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2, random_state=41)"
      ],
      "metadata": {
        "id": "eeCtLmH37HXE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit the model\n",
        "\n",
        "classifier=DecisionTreeClassifier(min_samples_split=2, max_depth=30)\n",
        "classifier.fit(X_train,Y_train)\n",
        "classifier.print_tree()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIbTHcvS7HVC",
        "outputId": "2825a1cf-d035-467e-fb0f-46d3a967f3a9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_2 <= 1.7 ? 0.33904421497105636\n",
            " left:Iris-setosa\n",
            " right:X_3 <= 1.5 ? 0.40269559500328744\n",
            "  left:X_2 <= 4.9 ? 0.04996712689020377\n",
            "    left:Iris-versicolor\n",
            "    right:Iris-virginica\n",
            "  right:X_2 <= 4.8 ? 0.040912933220625364\n",
            "    left:X_1 <= 2.8 ? 0.5\n",
            "        left:Iris-virginica\n",
            "        right:Iris-versicolor\n",
            "    right:X_3 <= 1.7 ? 0.026938775510204082\n",
            "        left:X_0 <= 6.7 ? 0.5\n",
            "                left:Iris-versicolor\n",
            "                right:Iris-virginica\n",
            "        right:Iris-virginica\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test the model\n",
        "\n",
        "Y_pred=classifier.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(Y_test, Y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtkmd9RE7HSy",
        "outputId": "c1f32765-19cb-4f35-d6b7-be662a3ab1c5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XJ6XE2uO7HQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "edWhH-Lh7HOR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}